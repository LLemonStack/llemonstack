version: 0.2.0 # Config file version

service: ollama
name: Ollama
description: LLM server for running models locally

compose_file: docker-compose.yaml
service_group: apps

provides:
  ollama: ollama # Name of the container

exposes:
  host:
    ollama:
      name: Ollama
      url: http://localhost:11434
  internal:
    ollama:
      name: Ollama
      url: ${OLLAMA_HOST:-http://host.docker.internal:11434}
