# LightRAG Settings

### Server Configuration
WEBUI_TITLE='LightRAG Engine'
WEBUI_DESCRIPTION="Simple and Fast Graph Based RAG System"
# HOST=0.0.0.0
# PORT=9621
# WORKERS=2
# CORS_ORIGINS=http://localhost:3000,http://localhost:8080

### Data storage: postgres + neo4j
LIGHTRAG_KV_STORAGE=PGKVStorage
LIGHTRAG_VECTOR_STORAGE=PGVectorStorage
LIGHTRAG_DOC_STATUS_STORAGE=PGDocStatusStorage
LIGHTRAG_GRAPH_STORAGE=Neo4JStorage

### Settings for RAG query
# HISTORY_TURNS=3
# COSINE_THRESHOLD=0.2
# TOP_K=60
# MAX_TOKEN_TEXT_CHUNK=4000
# MAX_TOKEN_RELATION_DESC=4000
# MAX_TOKEN_ENTITY_DESC=4000

### Settings for document indexing
SUMMARY_LANGUAGE=English
# CHUNK_SIZE=1200
# CHUNK_OVERLAP_SIZE=100

### Number of parallel processing documents in one patch
# MAX_PARALLEL_INSERT=2

### Max tokens for entity/relations description after merge
# MAX_TOKEN_SUMMARY=500
### Number of entities/edges to trigger LLM re-summary on merge ( at least 3 is recommented)
# FORCE_LLM_SUMMARY_ON_MERGE=6

### LLM Configuration
# Time out in seconds for LLM, None for infinite timeout
TIMEOUT=150
# Some models like o1-mini require temperature to be set to 1
TEMPERATURE=0.5
# Max concurrency requests of LLM
MAX_ASYNC=4
# Max tokens sent to LLM (less than context size of the model)
MAX_TOKENS=999999 # gpt-4.1 supports 1M tokens
# MAX_TOKENS=32768 # Default

### LLM Cache
ENABLE_LLM_CACHE=true
ENABLE_LLM_CACHE_FOR_EXTRACT=true

### OpenAI LLM
LLM_BINDING=${LIGHTRAG_LLM_BINDING:-openai} # lollms, ollama, openai, azure_openai
LLM_MODEL=${LIGHTRAG_LLM_MODEL:-gpt-4.1}
LLM_BINDING_HOST=${LIGHTRAG_LLM_BINDING_HOST:-https://api.openai.com/v1}
LLM_BINDING_API_KEY=${LIGHTRAG_LLM_BINDING_API_KEY:-${OPENAI_API_KEY}}

### Embedding
EMBEDDING_BINDING=${LIGHTRAG_EMBEDDING_BINDING:-openai}
EMBEDDING_BINDING_HOST=${LIGHTRAG_EMBEDDING_BINDING_HOST:-${LIGHTRAG_LLM_BINDING_HOST:-https://api.openai.com/v1}}
EMBEDDING_DIM=${LIGHTRAG_EMBEDDING_DIM:-1536} # Make sure this matches the embedding model dimensions
EMBEDDING_MODEL=${LIGHTRAG_EMBEDDING_MODEL:-text-embedding-3-small} # $0.02/1M tokens: 1536 dimensions
# See https://platform.openai.com/docs/pricing
# text-embedding-3-large # $0.13/1M tokens: 3072 dimensions
EMBEDDING_BINDING_API_KEY=${LIGHTRAG_EMBEDDING_BINDING_API_KEY:-${LIGHTRAG_LLM_BINDING_API_KEY:-${OPENAI_API_KEY}}}

### Num of chunks send to Embedding in single request
# EMBEDDING_BATCH_NUM=32
### Max concurrency requests for Embedding
# EMBEDDING_FUNC_MAX_ASYNC=16
# MAX_EMBED_TOKENS=8192

### PostgreSQL Configuration
POSTGRES_HOST=${LIGHTRAG_POSTGRES_HOST:-supavisor}
POSTGRES_PORT=${LIGHTRAG_POSTGRES_PORT:-5432}
POSTGRES_USER=${LIGHTRAG_POSTGRES_USER}${POSTGRES_USER_TENANT_SUFFIX}
POSTGRES_PASSWORD=${LIGHTRAG_POSTGRES_PASSWORD}
POSTGRES_DATABASE=${LIGHTRAG_POSTGRES_DATABASE:-postgres}

### Neo4j Configuration
NEO4J_URI=${LIGHTRAG_NEO4J_URI:-bolt://neo4j:7687}
NEO4J_USERNAME=${LIGHTRAG_NEO4J_USER}
NEO4J_PASSWORD=${LIGHTRAG_NEO4J_PASSWORD}

### JWT Auth
# AUTH_ACCOUNTS='admin:admin123,user1:pass456'
# TOKEN_SECRET=Your-Key-For-LightRAG-API-Server
# TOKEN_EXPIRE_HOURS=48
# GUEST_TOKEN_EXPIRE_HOURS=24
# JWT_ALGORITHM=HS256

### API-Key to access LightRAG Server API
LIGHTRAG_API_KEY=${LIGHTRAG_API_KEY:-}
WHITELIST_PATHS=/health,/login,/auth_status

### Optional SSL Configuration
# SSL=true
# SSL_CERTFILE=/path/to/cert.pem
# SSL_KEYFILE=/path/to/key.pem

### Directory Configuration (defaults to current working directory)
# WORKING_DIR=<absolute_path_for_working_dir>
# INPUT_DIR=<absolute_path_for_doc_input_dir>

### Ollama Emulating Model Tag
# OLLAMA_EMULATING_MODEL_TAG=latest

### Max nodes return from grap retrieval
# MAX_GRAPH_NODES=1000

### Logging level
# LOG_LEVEL=INFO
# VERBOSE=False
# LOG_MAX_BYTES=10485760
# LOG_BACKUP_COUNT=5
### Logfile location (defaults to current working directory)
# LOG_DIR=/path/to/log/directory
