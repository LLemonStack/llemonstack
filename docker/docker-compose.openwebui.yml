# Open Web UI for Ollama
#
# See https://docs.openwebui.com/

volumes:
  open-webui:
    driver: local

services:
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui
    labels:
      dev.dozzle.group: Local LLM
    restart: unless-stopped
    ports:
      - "8080:8080"
    extra_hosts:
      - "host.docker.internal:host-gateway"
    volumes:
      - open-webui:/app/backend/data
    environment:
      - GLOBAL_LOG_LEVEL=${OPENWEBUI_GLOBAL_LOG_LEVEL:-info}
      - OPENAI_API_BASE_URL=${OPENWEBUI_OPENAI_API_BASE_URL:-}
      - OPENAI_API_KEY=${OPENWEBUI_OPENAI_API_KEY:-}
      # - OLLAMA_BASE_URL=/ollama
      # - ENV=prod
      # - PORT=8080
      # - USE_OLLAMA_DOCKER=false
      # - USE_CUDA_DOCKER=false
      # - USE_CUDA_DOCKER_VER=cu121
      # - USE_EMBEDDING_MODEL_DOCKER=sentence-transformers/all-MiniLM-L6-v2
      # - WEBUI_SECRET_KEY=${WEBUI_SECRET_KEY}
      # - SCARF_NO_ANALYTICS=true
      # - DO_NOT_TRACK=true
      # - ANONYMIZED_TELEMETRY=false
      # - WHISPER_MODEL=base
      # - WHISPER_MODEL_DIR=/app/backend/data/cache/whisper/models
      # - RAG_EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
      # - RAG_RERANKING_MODEL=
      # - SENTENCE_TRANSFORMERS_HOME=/app/backend/data/cache/embedding/models
      # - TIKTOKEN_ENCODING_NAME=cl100k_base
      # - TIKTOKEN_CACHE_DIR=/app/backend/data/cache/tiktoken
      # - DOCKER=true
      # - CORS_ALLOW_ORIGIN=*
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "curl --silent --fail http://localhost:8080/health | jq -ne 'input.status == true' || exit 1",
        ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 90s
